{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from efficientnet import EfficientNet_B0\n",
    "\n",
    "from torchvision import transforms\n",
    "from utils import *\n",
    "from voc_dataloader import VocDataset, VOC_CLASSES\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\498_working\\efficientN\\voc_dataloader.py:109: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(names), np.array(labels).astype(np.float32), np.array(box_indices), label_order\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std= [0.229, 0.224, 0.225])\n",
    "\n",
    "# Use random crop and flip for training\n",
    "train_transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224,scale = (0.3,1.0)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize\n",
    "        ])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "            transforms.Resize(224),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "\n",
    "ds_train = VocDataset('../VOCdevkit_2007/VOC2007/','train',train_transform)\n",
    "ds_val = VocDataset('../VOCdevkit_2007/VOC2007/','val',test_transform)\n",
    "ds_test = VocDataset('../VOCdevkit_2007/VOC2007test/','test', test_transform)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "test_frequency = 5\n",
    "batch_size = 36\n",
    "# set a higher number of workers for better performance\n",
    "num_workers = 8\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=ds_train,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True,\n",
    "                                            pin_memory=True,\n",
    "                                            num_workers=num_workers)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=ds_val,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True,\n",
    "                                            pin_memory=True,\n",
    "                                            num_workers=num_workers)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=ds_test,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=False,\n",
    "                                               pin_memory=True,\n",
    "                                               num_workers=num_workers)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Efficient Net B0\n",
    "\n",
    "started 1:39"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch number 1\n",
      "Loss for Training on Epoch 1 is 0.25039950013160706\n",
      "-------  Class: aeroplane        AP:   0.0728  -------\n",
      "-------  Class: bicycle          AP:   0.0619  -------\n",
      "-------  Class: bird             AP:   0.1064  -------\n",
      "-------  Class: boat             AP:   0.1138  -------\n",
      "-------  Class: bottle           AP:   0.0712  -------\n",
      "-------  Class: bus              AP:   0.0500  -------\n",
      "-------  Class: car              AP:   0.1875  -------\n",
      "-------  Class: cat              AP:   0.1526  -------\n",
      "-------  Class: chair            AP:   0.1858  -------\n",
      "-------  Class: cow              AP:   0.0393  -------\n",
      "-------  Class: diningtable      AP:   0.1460  -------\n",
      "-------  Class: dog              AP:   0.1318  -------\n",
      "-------  Class: horse            AP:   0.0751  -------\n",
      "-------  Class: motorbike        AP:   0.0698  -------\n",
      "-------  Class: person           AP:   0.4189  -------\n",
      "-------  Class: pottedplant      AP:   0.0490  -------\n",
      "-------  Class: sheep            AP:   0.0671  -------\n",
      "-------  Class: sofa             AP:   0.1639  -------\n",
      "-------  Class: train            AP:   0.0674  -------\n",
      "-------  Class: tvmonitor        AP:   0.0556  -------\n",
      "mAP: 0.1143\n",
      "Avg loss: 0.5275249362318402\n",
      "Evaluating classifier\n",
      "Mean Precision Score for Testing on Epoch 1 is 0.11429204864583646\n",
      "Starting epoch number 2\n",
      "Loss for Training on Epoch 2 is 0.23123766481876373\n",
      "Starting epoch number 3\n",
      "Loss for Training on Epoch 3 is 0.22844864428043365\n",
      "Starting epoch number 4\n",
      "Loss for Training on Epoch 4 is 0.22605304419994354\n",
      "Starting epoch number 5\n",
      "Loss for Training on Epoch 5 is 0.22461852431297302\n",
      "-------  Class: aeroplane        AP:   0.2833  -------\n",
      "-------  Class: bicycle          AP:   0.0840  -------\n",
      "-------  Class: bird             AP:   0.1287  -------\n",
      "-------  Class: boat             AP:   0.1570  -------\n",
      "-------  Class: bottle           AP:   0.0974  -------\n",
      "-------  Class: bus              AP:   0.0769  -------\n",
      "-------  Class: car              AP:   0.2827  -------\n",
      "-------  Class: cat              AP:   0.1618  -------\n",
      "-------  Class: chair            AP:   0.2751  -------\n",
      "-------  Class: cow              AP:   0.0726  -------\n",
      "-------  Class: diningtable      AP:   0.1848  -------\n",
      "-------  Class: dog              AP:   0.1392  -------\n",
      "-------  Class: horse            AP:   0.1479  -------\n",
      "-------  Class: motorbike        AP:   0.0841  -------\n",
      "-------  Class: person           AP:   0.4861  -------\n",
      "-------  Class: pottedplant      AP:   0.1002  -------\n",
      "-------  Class: sheep            AP:   0.1163  -------\n",
      "-------  Class: sofa             AP:   0.1473  -------\n",
      "-------  Class: train            AP:   0.1228  -------\n",
      "-------  Class: tvmonitor        AP:   0.0799  -------\n",
      "mAP: 0.1614\n",
      "Avg loss: 0.2296515155212575\n",
      "Evaluating classifier\n",
      "Mean Precision Score for Testing on Epoch 5 is 0.16140897983963326\n",
      "Starting epoch number 6\n",
      "Loss for Training on Epoch 6 is 0.22324922680854797\n",
      "Starting epoch number 7\n",
      "Loss for Training on Epoch 7 is 0.2217441350221634\n",
      "Starting epoch number 8\n",
      "Loss for Training on Epoch 8 is 0.22125037014484406\n",
      "Starting epoch number 9\n",
      "Loss for Training on Epoch 9 is 0.22037416696548462\n",
      "Starting epoch number 10\n",
      "Loss for Training on Epoch 10 is 0.2178705483675003\n",
      "-------  Class: aeroplane        AP:   0.2249  -------\n",
      "-------  Class: bicycle          AP:   0.1010  -------\n",
      "-------  Class: bird             AP:   0.1319  -------\n",
      "-------  Class: boat             AP:   0.1076  -------\n",
      "-------  Class: bottle           AP:   0.0794  -------\n",
      "-------  Class: bus              AP:   0.0840  -------\n",
      "-------  Class: car              AP:   0.2445  -------\n",
      "-------  Class: cat              AP:   0.2143  -------\n",
      "-------  Class: chair            AP:   0.2913  -------\n",
      "-------  Class: cow              AP:   0.0810  -------\n",
      "-------  Class: diningtable      AP:   0.1857  -------\n",
      "-------  Class: dog              AP:   0.1989  -------\n",
      "-------  Class: horse            AP:   0.2231  -------\n",
      "-------  Class: motorbike        AP:   0.0719  -------\n",
      "-------  Class: person           AP:   0.5052  -------\n",
      "-------  Class: pottedplant      AP:   0.1311  -------\n",
      "-------  Class: sheep            AP:   0.0914  -------\n",
      "-------  Class: sofa             AP:   0.2118  -------\n",
      "-------  Class: train            AP:   0.0946  -------\n",
      "-------  Class: tvmonitor        AP:   0.0803  -------\n",
      "mAP: 0.1677\n",
      "Avg loss: 0.26059420247269105\n",
      "Evaluating classifier\n",
      "Mean Precision Score for Testing on Epoch 10 is 0.16769523167525024\n",
      "Starting epoch number 11\n",
      "Loss for Training on Epoch 11 is 0.21655380725860596\n",
      "Starting epoch number 12\n",
      "Loss for Training on Epoch 12 is 0.2157888412475586\n",
      "Starting epoch number 13\n",
      "Loss for Training on Epoch 13 is 0.21478144824504852\n",
      "Starting epoch number 14\n",
      "Loss for Training on Epoch 14 is 0.21353524923324585\n",
      "Starting epoch number 15\n",
      "Loss for Training on Epoch 15 is 0.21213246881961823\n",
      "-------  Class: aeroplane        AP:   0.2678  -------\n",
      "-------  Class: bicycle          AP:   0.0919  -------\n",
      "-------  Class: bird             AP:   0.1371  -------\n",
      "-------  Class: boat             AP:   0.0773  -------\n",
      "-------  Class: bottle           AP:   0.0816  -------\n",
      "-------  Class: bus              AP:   0.0942  -------\n",
      "-------  Class: car              AP:   0.2891  -------\n",
      "-------  Class: cat              AP:   0.1776  -------\n",
      "-------  Class: chair            AP:   0.2217  -------\n",
      "-------  Class: cow              AP:   0.0822  -------\n",
      "-------  Class: diningtable      AP:   0.1154  -------\n",
      "-------  Class: dog              AP:   0.2101  -------\n",
      "-------  Class: horse            AP:   0.1692  -------\n",
      "-------  Class: motorbike        AP:   0.0978  -------\n",
      "-------  Class: person           AP:   0.5306  -------\n",
      "-------  Class: pottedplant      AP:   0.0991  -------\n",
      "-------  Class: sheep            AP:   0.0757  -------\n",
      "-------  Class: sofa             AP:   0.1507  -------\n",
      "-------  Class: train            AP:   0.1488  -------\n",
      "-------  Class: tvmonitor        AP:   0.1011  -------\n",
      "mAP: 0.1609\n",
      "Avg loss: 0.2406124347026842\n",
      "Evaluating classifier\n",
      "Mean Precision Score for Testing on Epoch 15 is 0.16094149591600374\n",
      "Starting epoch number 16\n",
      "Loss for Training on Epoch 16 is 0.2117580622434616\n",
      "Starting epoch number 17\n",
      "Loss for Training on Epoch 17 is 0.2091744989156723\n",
      "Starting epoch number 18\n",
      "Loss for Training on Epoch 18 is 0.20827259123325348\n",
      "Starting epoch number 19\n",
      "Loss for Training on Epoch 19 is 0.20634427666664124\n",
      "Starting epoch number 20\n",
      "Loss for Training on Epoch 20 is 0.20656326413154602\n",
      "-------  Class: aeroplane        AP:   0.3711  -------\n",
      "-------  Class: bicycle          AP:   0.1269  -------\n",
      "-------  Class: bird             AP:   0.1562  -------\n",
      "-------  Class: boat             AP:   0.2122  -------\n",
      "-------  Class: bottle           AP:   0.0929  -------\n",
      "-------  Class: bus              AP:   0.1525  -------\n",
      "-------  Class: car              AP:   0.3424  -------\n",
      "-------  Class: cat              AP:   0.2199  -------\n",
      "-------  Class: chair            AP:   0.3628  -------\n",
      "-------  Class: cow              AP:   0.1034  -------\n",
      "-------  Class: diningtable      AP:   0.1541  -------\n",
      "-------  Class: dog              AP:   0.2682  -------\n",
      "-------  Class: horse            AP:   0.2370  -------\n",
      "-------  Class: motorbike        AP:   0.1396  -------\n",
      "-------  Class: person           AP:   0.5692  -------\n",
      "-------  Class: pottedplant      AP:   0.1718  -------\n",
      "-------  Class: sheep            AP:   0.1054  -------\n",
      "-------  Class: sofa             AP:   0.2042  -------\n",
      "-------  Class: train            AP:   0.2841  -------\n",
      "-------  Class: tvmonitor        AP:   0.1436  -------\n",
      "mAP: 0.2209\n",
      "Avg loss: 0.22567728343475388\n",
      "Evaluating classifier\n",
      "Mean Precision Score for Testing on Epoch 20 is 0.2208760310654226\n",
      "Starting epoch number 21\n",
      "Loss for Training on Epoch 21 is 0.20398566126823425\n",
      "Starting epoch number 22\n",
      "Loss for Training on Epoch 22 is 0.20410481095314026\n",
      "Starting epoch number 23\n",
      "Loss for Training on Epoch 23 is 0.2027406394481659\n",
      "Starting epoch number 24\n",
      "Loss for Training on Epoch 24 is 0.19969046115875244\n",
      "Starting epoch number 25\n",
      "Loss for Training on Epoch 25 is 0.20037002861499786\n",
      "-------  Class: aeroplane        AP:   0.3428  -------\n",
      "-------  Class: bicycle          AP:   0.1705  -------\n",
      "-------  Class: bird             AP:   0.1551  -------\n",
      "-------  Class: boat             AP:   0.1607  -------\n",
      "-------  Class: bottle           AP:   0.0876  -------\n",
      "-------  Class: bus              AP:   0.1424  -------\n",
      "-------  Class: car              AP:   0.3360  -------\n",
      "-------  Class: cat              AP:   0.2190  -------\n",
      "-------  Class: chair            AP:   0.3959  -------\n",
      "-------  Class: cow              AP:   0.1139  -------\n",
      "-------  Class: diningtable      AP:   0.1633  -------\n",
      "-------  Class: dog              AP:   0.2279  -------\n",
      "-------  Class: horse            AP:   0.2584  -------\n",
      "-------  Class: motorbike        AP:   0.1540  -------\n",
      "-------  Class: person           AP:   0.5821  -------\n",
      "-------  Class: pottedplant      AP:   0.1495  -------\n",
      "-------  Class: sheep            AP:   0.1417  -------\n",
      "-------  Class: sofa             AP:   0.1837  -------\n",
      "-------  Class: train            AP:   0.3053  -------\n",
      "-------  Class: tvmonitor        AP:   0.2003  -------\n",
      "mAP: 0.2245\n",
      "Avg loss: 0.23112946614266866\n",
      "Evaluating classifier\n",
      "Mean Precision Score for Testing on Epoch 25 is 0.2245043402668911\n",
      "Starting epoch number 26\n",
      "Loss for Training on Epoch 26 is 0.1996188759803772\n",
      "Starting epoch number 27\n",
      "Loss for Training on Epoch 27 is 0.19881848990917206\n",
      "Starting epoch number 28\n",
      "Loss for Training on Epoch 28 is 0.19659292697906494\n",
      "Starting epoch number 29\n",
      "Loss for Training on Epoch 29 is 0.19701257348060608\n",
      "Starting epoch number 30\n",
      "Loss for Training on Epoch 30 is 0.19562625885009766\n",
      "-------  Class: aeroplane        AP:   0.3645  -------\n",
      "-------  Class: bicycle          AP:   0.1872  -------\n",
      "-------  Class: bird             AP:   0.1642  -------\n",
      "-------  Class: boat             AP:   0.2941  -------\n",
      "-------  Class: bottle           AP:   0.0911  -------\n",
      "-------  Class: bus              AP:   0.1537  -------\n",
      "-------  Class: car              AP:   0.3307  -------\n",
      "-------  Class: cat              AP:   0.2608  -------\n",
      "-------  Class: chair            AP:   0.3678  -------\n",
      "-------  Class: cow              AP:   0.1053  -------\n",
      "-------  Class: diningtable      AP:   0.1542  -------\n",
      "-------  Class: dog              AP:   0.2476  -------\n",
      "-------  Class: horse            AP:   0.2567  -------\n",
      "-------  Class: motorbike        AP:   0.1901  -------\n",
      "-------  Class: person           AP:   0.5738  -------\n",
      "-------  Class: pottedplant      AP:   0.1093  -------\n",
      "-------  Class: sheep            AP:   0.1494  -------\n",
      "-------  Class: sofa             AP:   0.1799  -------\n",
      "-------  Class: train            AP:   0.3982  -------\n",
      "-------  Class: tvmonitor        AP:   0.2482  -------\n",
      "mAP: 0.2413\n",
      "Avg loss: 0.23539838513500425\n",
      "Evaluating classifier\n",
      "Mean Precision Score for Testing on Epoch 30 is 0.24132443188322839\n",
      "Starting epoch number 31\n",
      "Loss for Training on Epoch 31 is 0.19585245847702026\n",
      "Starting epoch number 32\n",
      "Loss for Training on Epoch 32 is 0.1957479864358902\n",
      "Starting epoch number 33\n",
      "Loss for Training on Epoch 33 is 0.19460690021514893\n",
      "Starting epoch number 34\n",
      "Loss for Training on Epoch 34 is 0.1943047046661377\n",
      "Starting epoch number 35\n",
      "Loss for Training on Epoch 35 is 0.19488714635372162\n",
      "-------  Class: aeroplane        AP:   0.2817  -------\n",
      "-------  Class: bicycle          AP:   0.1457  -------\n",
      "-------  Class: bird             AP:   0.1706  -------\n",
      "-------  Class: boat             AP:   0.1659  -------\n",
      "-------  Class: bottle           AP:   0.0882  -------\n",
      "-------  Class: bus              AP:   0.1046  -------\n",
      "-------  Class: car              AP:   0.2691  -------\n",
      "-------  Class: cat              AP:   0.2594  -------\n",
      "-------  Class: chair            AP:   0.3550  -------\n",
      "-------  Class: cow              AP:   0.1179  -------\n",
      "-------  Class: diningtable      AP:   0.1644  -------\n",
      "-------  Class: dog              AP:   0.2657  -------\n",
      "-------  Class: horse            AP:   0.2835  -------\n",
      "-------  Class: motorbike        AP:   0.1586  -------\n",
      "-------  Class: person           AP:   0.5734  -------\n",
      "-------  Class: pottedplant      AP:   0.1080  -------\n",
      "-------  Class: sheep            AP:   0.1867  -------\n",
      "-------  Class: sofa             AP:   0.1606  -------\n",
      "-------  Class: train            AP:   0.4007  -------\n",
      "-------  Class: tvmonitor        AP:   0.1554  -------\n",
      "mAP: 0.2208\n",
      "Avg loss: 0.2437593311783802\n",
      "Evaluating classifier\n",
      "Mean Precision Score for Testing on Epoch 35 is 0.22075438767696814\n",
      "Starting epoch number 36\n",
      "Loss for Training on Epoch 36 is 0.19288504123687744\n",
      "Starting epoch number 37\n",
      "Loss for Training on Epoch 37 is 0.19100603461265564\n",
      "Starting epoch number 38\n",
      "Loss for Training on Epoch 38 is 0.19167716801166534\n",
      "Starting epoch number 39\n",
      "Loss for Training on Epoch 39 is 0.19116759300231934\n",
      "Starting epoch number 40\n",
      "Loss for Training on Epoch 40 is 0.1902303844690323\n",
      "-------  Class: aeroplane        AP:   0.2371  -------\n",
      "-------  Class: bicycle          AP:   0.1880  -------\n",
      "-------  Class: bird             AP:   0.1725  -------\n",
      "-------  Class: boat             AP:   0.2365  -------\n",
      "-------  Class: bottle           AP:   0.0851  -------\n",
      "-------  Class: bus              AP:   0.1236  -------\n",
      "-------  Class: car              AP:   0.2677  -------\n",
      "-------  Class: cat              AP:   0.2262  -------\n",
      "-------  Class: chair            AP:   0.3614  -------\n",
      "-------  Class: cow              AP:   0.0888  -------\n",
      "-------  Class: diningtable      AP:   0.1624  -------\n",
      "-------  Class: dog              AP:   0.1987  -------\n",
      "-------  Class: horse            AP:   0.2737  -------\n",
      "-------  Class: motorbike        AP:   0.1519  -------\n",
      "-------  Class: person           AP:   0.5810  -------\n",
      "-------  Class: pottedplant      AP:   0.1274  -------\n",
      "-------  Class: sheep            AP:   0.1212  -------\n",
      "-------  Class: sofa             AP:   0.1564  -------\n",
      "-------  Class: train            AP:   0.3144  -------\n",
      "-------  Class: tvmonitor        AP:   0.1494  -------\n",
      "mAP: 0.2112\n",
      "Avg loss: 0.25978512447401586\n",
      "Evaluating classifier\n",
      "Mean Precision Score for Testing on Epoch 40 is 0.2111653075419016\n",
      "Starting epoch number 41\n",
      "Loss for Training on Epoch 41 is 0.19041471183300018\n",
      "Starting epoch number 42\n",
      "Loss for Training on Epoch 42 is 0.19034408032894135\n",
      "Starting epoch number 43\n",
      "Loss for Training on Epoch 43 is 0.18837516009807587\n",
      "Starting epoch number 44\n",
      "Loss for Training on Epoch 44 is 0.18772757053375244\n",
      "Starting epoch number 45\n",
      "Loss for Training on Epoch 45 is 0.18825620412826538\n",
      "-------  Class: aeroplane        AP:   0.4015  -------\n",
      "-------  Class: bicycle          AP:   0.2163  -------\n",
      "-------  Class: bird             AP:   0.2043  -------\n",
      "-------  Class: boat             AP:   0.1820  -------\n",
      "-------  Class: bottle           AP:   0.1099  -------\n",
      "-------  Class: bus              AP:   0.1598  -------\n",
      "-------  Class: car              AP:   0.3293  -------\n",
      "-------  Class: cat              AP:   0.2470  -------\n",
      "-------  Class: chair            AP:   0.3613  -------\n",
      "-------  Class: cow              AP:   0.1051  -------\n",
      "-------  Class: diningtable      AP:   0.1850  -------\n",
      "-------  Class: dog              AP:   0.1838  -------\n",
      "-------  Class: horse            AP:   0.3419  -------\n",
      "-------  Class: motorbike        AP:   0.1973  -------\n",
      "-------  Class: person           AP:   0.5715  -------\n",
      "-------  Class: pottedplant      AP:   0.1537  -------\n",
      "-------  Class: sheep            AP:   0.1604  -------\n",
      "-------  Class: sofa             AP:   0.1452  -------\n",
      "-------  Class: train            AP:   0.3787  -------\n",
      "-------  Class: tvmonitor        AP:   0.2319  -------\n",
      "mAP: 0.2433\n",
      "Avg loss: 0.24654983439115413\n",
      "Evaluating classifier\n",
      "Mean Precision Score for Testing on Epoch 45 is 0.24330086955192196\n",
      "Starting epoch number 46\n",
      "Loss for Training on Epoch 46 is 0.18602794408798218\n",
      "Starting epoch number 47\n",
      "Loss for Training on Epoch 47 is 0.18658746778964996\n",
      "Starting epoch number 48\n",
      "Loss for Training on Epoch 48 is 0.1843574494123459\n",
      "Starting epoch number 49\n",
      "Loss for Training on Epoch 49 is 0.18558229506015778\n",
      "Starting epoch number 50\n",
      "Loss for Training on Epoch 50 is 0.1855027675628662\n",
      "-------  Class: aeroplane        AP:   0.3036  -------\n",
      "-------  Class: bicycle          AP:   0.1039  -------\n",
      "-------  Class: bird             AP:   0.1658  -------\n",
      "-------  Class: boat             AP:   0.1584  -------\n",
      "-------  Class: bottle           AP:   0.0920  -------\n",
      "-------  Class: bus              AP:   0.1288  -------\n",
      "-------  Class: car              AP:   0.4211  -------\n",
      "-------  Class: cat              AP:   0.2490  -------\n",
      "-------  Class: chair            AP:   0.4225  -------\n",
      "-------  Class: cow              AP:   0.0904  -------\n",
      "-------  Class: diningtable      AP:   0.2505  -------\n",
      "-------  Class: dog              AP:   0.2177  -------\n",
      "-------  Class: horse            AP:   0.1993  -------\n",
      "-------  Class: motorbike        AP:   0.1361  -------\n",
      "-------  Class: person           AP:   0.5511  -------\n",
      "-------  Class: pottedplant      AP:   0.1646  -------\n",
      "-------  Class: sheep            AP:   0.0833  -------\n",
      "-------  Class: sofa             AP:   0.1964  -------\n",
      "-------  Class: train            AP:   0.3847  -------\n",
      "-------  Class: tvmonitor        AP:   0.2508  -------\n",
      "mAP: 0.2285\n",
      "Avg loss: 0.3020817321083282\n",
      "Evaluating classifier\n",
      "Mean Precision Score for Testing on Epoch 50 is 0.22850584432749327\n",
      "Starting epoch number 51\n",
      "Loss for Training on Epoch 51 is 0.18517200648784637\n",
      "Starting epoch number 52\n",
      "Loss for Training on Epoch 52 is 0.18509364128112793\n",
      "Starting epoch number 53\n",
      "Loss for Training on Epoch 53 is 0.1848476231098175\n",
      "Starting epoch number 54\n",
      "Loss for Training on Epoch 54 is 0.18412865698337555\n",
      "Starting epoch number 55\n",
      "Loss for Training on Epoch 55 is 0.18266326189041138\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-8-f48633a25d41>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[0mclassifier\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_losses\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mval_losses\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_mAPs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mval_mAPs\u001B[0m \u001B[1;33m=\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 8\u001B[1;33m     \u001B[0mtrain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mclassifier\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnum_epochs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_loader\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mval_loader\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtest_frequency\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      9\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-5-d7c1b2241e4e>\u001B[0m in \u001B[0;36mtrain\u001B[1;34m(classifier, num_epochs, train_loader, val_loader, criterion, optimizer, test_frequency)\u001B[0m\n\u001B[0;32m     13\u001B[0m             \u001B[0mmAP_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtest_classifier\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_loader\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mclassifier\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mFalse\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     14\u001B[0m             \u001B[0mtrain_mAPs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmAP_train\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 15\u001B[1;33m             \u001B[0mmAP_val\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mval_loss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtest_classifier\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mval_loader\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mclassifier\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     16\u001B[0m             \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'Evaluating classifier'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     17\u001B[0m             \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Mean Precision Score for Testing on Epoch \"\u001B[0m \u001B[1;33m+\u001B[0m\u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mepoch\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;34m\" is \"\u001B[0m\u001B[1;33m+\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmAP_val\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-3-5cef480a8d83>\u001B[0m in \u001B[0;36mtest_classifier\u001B[1;34m(test_loader, classifier, criterion, print_ind_classes, print_total)\u001B[0m\n\u001B[0;32m      8\u001B[0m             \u001B[0mimages\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabels\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mimages\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabels\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m             \u001B[0mlogits\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mclassifier\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimages\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 10\u001B[1;33m             \u001B[0my_true\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconcatenate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my_true\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabels\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcpu\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     11\u001B[0m             \u001B[0my_score\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconcatenate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my_score\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlogits\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcpu\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     12\u001B[0m             \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlogits\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabels\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "classifier = EfficientNet_B0().to(device)\n",
    "\n",
    "criterion = nn.MultiLabelSoftMarginLoss()\n",
    "#optimizer = torch.optim.SGD(classifier.parameters(), lr=0.01, momentum=0.9)\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-4)\n",
    "\n",
    "classifier, train_losses, val_losses, train_mAPs, val_mAPs = \\\n",
    "    train(classifier, num_epochs, train_loader, val_loader, criterion, optimizer, test_frequency)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "ori = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "ori._fc = nn.Linear(1280,21)\n",
    "ori = ori.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch number 1\n",
      "Loss for Training on Epoch 1 is 0.4731805622577667\n",
      "-------  Class: aeroplane        AP:   0.0923  -------\n",
      "-------  Class: bicycle          AP:   0.1192  -------\n",
      "-------  Class: bird             AP:   0.1957  -------\n",
      "-------  Class: boat             AP:   0.0681  -------\n",
      "-------  Class: bottle           AP:   0.0810  -------\n",
      "-------  Class: bus              AP:   0.0485  -------\n",
      "-------  Class: car              AP:   0.2938  -------\n",
      "-------  Class: cat              AP:   0.1343  -------\n",
      "-------  Class: chair            AP:   0.2284  -------\n",
      "-------  Class: cow              AP:   0.0297  -------\n",
      "-------  Class: diningtable      AP:   0.1066  -------\n",
      "-------  Class: dog              AP:   0.3090  -------\n",
      "-------  Class: horse            AP:   0.1550  -------\n",
      "-------  Class: motorbike        AP:   0.1064  -------\n",
      "-------  Class: person           AP:   0.8242  -------\n",
      "-------  Class: pottedplant      AP:   0.0512  -------\n",
      "-------  Class: sheep            AP:   0.0153  -------\n",
      "-------  Class: sofa             AP:   0.1060  -------\n",
      "-------  Class: train            AP:   0.1558  -------\n",
      "-------  Class: tvmonitor        AP:   0.0649  -------\n",
      "mAP: 0.1593\n",
      "Avg loss: 0.296516241984708\n",
      "Evaluating classifier\n",
      "Mean Precision Score for Testing on Epoch 1 is 0.15927737951531692\n",
      "Starting epoch number 2\n",
      "Loss for Training on Epoch 2 is 0.21557669341564178\n",
      "Starting epoch number 3\n",
      "Loss for Training on Epoch 3 is 0.17162954807281494\n",
      "Starting epoch number 4\n",
      "Loss for Training on Epoch 4 is 0.14782655239105225\n",
      "Starting epoch number 5\n",
      "Loss for Training on Epoch 5 is 0.13125278055667877\n",
      "-------  Class: aeroplane        AP:   0.8571  -------\n",
      "-------  Class: bicycle          AP:   0.6921  -------\n",
      "-------  Class: bird             AP:   0.8887  -------\n",
      "-------  Class: boat             AP:   0.3140  -------\n",
      "-------  Class: bottle           AP:   0.3307  -------\n",
      "-------  Class: bus              AP:   0.3857  -------\n",
      "-------  Class: car              AP:   0.8643  -------\n",
      "-------  Class: cat              AP:   0.8959  -------\n",
      "-------  Class: chair            AP:   0.6455  -------\n",
      "-------  Class: cow              AP:   0.5090  -------\n",
      "-------  Class: diningtable      AP:   0.6137  -------\n",
      "-------  Class: dog              AP:   0.8740  -------\n",
      "-------  Class: horse            AP:   0.8507  -------\n",
      "-------  Class: motorbike        AP:   0.7839  -------\n",
      "-------  Class: person           AP:   0.9357  -------\n",
      "-------  Class: pottedplant      AP:   0.2716  -------\n",
      "-------  Class: sheep            AP:   0.3086  -------\n",
      "-------  Class: sofa             AP:   0.5661  -------\n",
      "-------  Class: train            AP:   0.8731  -------\n",
      "-------  Class: tvmonitor        AP:   0.4578  -------\n",
      "mAP: 0.6459\n",
      "Avg loss: 0.11880254777414458\n",
      "Evaluating classifier\n",
      "Mean Precision Score for Testing on Epoch 5 is 0.6459190669742095\n",
      "Starting epoch number 6\n",
      "Loss for Training on Epoch 6 is 0.12076584994792938\n",
      "Starting epoch number 7\n",
      "Loss for Training on Epoch 7 is 0.10840259492397308\n",
      "Starting epoch number 8\n",
      "Loss for Training on Epoch 8 is 0.09922026097774506\n",
      "Starting epoch number 9\n",
      "Loss for Training on Epoch 9 is 0.0935099869966507\n",
      "Starting epoch number 10\n",
      "Loss for Training on Epoch 10 is 0.08778641372919083\n",
      "-------  Class: aeroplane        AP:   0.9489  -------\n",
      "-------  Class: bicycle          AP:   0.8555  -------\n",
      "-------  Class: bird             AP:   0.9173  -------\n",
      "-------  Class: boat             AP:   0.8142  -------\n",
      "-------  Class: bottle           AP:   0.5283  -------\n",
      "-------  Class: bus              AP:   0.7286  -------\n",
      "-------  Class: car              AP:   0.9145  -------\n",
      "-------  Class: cat              AP:   0.9151  -------\n",
      "-------  Class: chair            AP:   0.6808  -------\n",
      "-------  Class: cow              AP:   0.6990  -------\n",
      "-------  Class: diningtable      AP:   0.7135  -------\n",
      "-------  Class: dog              AP:   0.9081  -------\n",
      "-------  Class: horse            AP:   0.8866  -------\n",
      "-------  Class: motorbike        AP:   0.9003  -------\n",
      "-------  Class: person           AP:   0.9473  -------\n",
      "-------  Class: pottedplant      AP:   0.5766  -------\n",
      "-------  Class: sheep            AP:   0.5674  -------\n",
      "-------  Class: sofa             AP:   0.6595  -------\n",
      "-------  Class: train            AP:   0.9541  -------\n",
      "-------  Class: tvmonitor        AP:   0.6951  -------\n",
      "mAP: 0.7905\n",
      "Avg loss: 0.09056247025728226\n",
      "Evaluating classifier\n",
      "Mean Precision Score for Testing on Epoch 10 is 0.790540280908465\n",
      "Starting epoch number 11\n",
      "Loss for Training on Epoch 11 is 0.08277022838592529\n",
      "Starting epoch number 12\n",
      "Loss for Training on Epoch 12 is 0.07677989453077316\n",
      "Starting epoch number 13\n",
      "Loss for Training on Epoch 13 is 0.07219073176383972\n",
      "Starting epoch number 14\n",
      "Loss for Training on Epoch 14 is 0.06862284988164902\n",
      "Starting epoch number 15\n",
      "Loss for Training on Epoch 15 is 0.06407129019498825\n",
      "-------  Class: aeroplane        AP:   0.9595  -------\n",
      "-------  Class: bicycle          AP:   0.8687  -------\n",
      "-------  Class: bird             AP:   0.9336  -------\n",
      "-------  Class: boat             AP:   0.8739  -------\n",
      "-------  Class: bottle           AP:   0.5716  -------\n",
      "-------  Class: bus              AP:   0.7658  -------\n",
      "-------  Class: car              AP:   0.9191  -------\n",
      "-------  Class: cat              AP:   0.9347  -------\n",
      "-------  Class: chair            AP:   0.6871  -------\n",
      "-------  Class: cow              AP:   0.7935  -------\n",
      "-------  Class: diningtable      AP:   0.7285  -------\n",
      "-------  Class: dog              AP:   0.9124  -------\n",
      "-------  Class: horse            AP:   0.9196  -------\n",
      "-------  Class: motorbike        AP:   0.8983  -------\n",
      "-------  Class: person           AP:   0.9488  -------\n",
      "-------  Class: pottedplant      AP:   0.6344  -------\n",
      "-------  Class: sheep            AP:   0.7276  -------\n",
      "-------  Class: sofa             AP:   0.6871  -------\n",
      "-------  Class: train            AP:   0.9626  -------\n",
      "-------  Class: tvmonitor        AP:   0.7267  -------\n",
      "mAP: 0.8227\n",
      "Avg loss: 0.08250103332102299\n",
      "Evaluating classifier\n",
      "Mean Precision Score for Testing on Epoch 15 is 0.8226793800008212\n",
      "Starting epoch number 16\n",
      "Loss for Training on Epoch 16 is 0.06360386312007904\n",
      "Starting epoch number 17\n",
      "Loss for Training on Epoch 17 is 0.057979289442300797\n",
      "Starting epoch number 18\n",
      "Loss for Training on Epoch 18 is 0.054864827543497086\n",
      "Starting epoch number 19\n",
      "Loss for Training on Epoch 19 is 0.051784634590148926\n",
      "Starting epoch number 20\n",
      "Loss for Training on Epoch 20 is 0.05058949440717697\n",
      "-------  Class: aeroplane        AP:   0.9578  -------\n",
      "-------  Class: bicycle          AP:   0.8756  -------\n",
      "-------  Class: bird             AP:   0.9414  -------\n",
      "-------  Class: boat             AP:   0.8869  -------\n",
      "-------  Class: bottle           AP:   0.5770  -------\n",
      "-------  Class: bus              AP:   0.7867  -------\n",
      "-------  Class: car              AP:   0.9147  -------\n",
      "-------  Class: cat              AP:   0.9278  -------\n",
      "-------  Class: chair            AP:   0.6753  -------\n",
      "-------  Class: cow              AP:   0.8138  -------\n",
      "-------  Class: diningtable      AP:   0.7370  -------\n",
      "-------  Class: dog              AP:   0.9098  -------\n",
      "-------  Class: horse            AP:   0.9262  -------\n",
      "-------  Class: motorbike        AP:   0.9025  -------\n",
      "-------  Class: person           AP:   0.9456  -------\n",
      "-------  Class: pottedplant      AP:   0.6558  -------\n",
      "-------  Class: sheep            AP:   0.7764  -------\n",
      "-------  Class: sofa             AP:   0.6581  -------\n",
      "-------  Class: train            AP:   0.9735  -------\n",
      "-------  Class: tvmonitor        AP:   0.7381  -------\n",
      "mAP: 0.8290\n",
      "Avg loss: 0.08341088646224566\n",
      "Evaluating classifier\n",
      "Mean Precision Score for Testing on Epoch 20 is 0.8290097515511073\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MultiLabelSoftMarginLoss()\n",
    "#optimizer = torch.optim.SGD(classifier.parameters(), lr=0.01, momentum=0.9)\n",
    "optimizer = torch.optim.Adam(ori.parameters(), lr=1e-4)\n",
    "\n",
    "ori, train_losses, val_losses, train_mAPs, val_mAPs = \\\n",
    "    train(ori, num_epochs, train_loader, val_loader, criterion, optimizer, test_frequency)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "Linear(in_features=1280, out_features=1000, bias=True)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "EfficientNet(\n  (_conv_stem): Conv2dStaticSamePadding(\n    3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n    (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n  )\n  (_bn0): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n  (_blocks): ModuleList(\n    (0): MBConvBlock(\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n      )\n      (_bn1): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        32, 8, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        8, 32, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(16, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (1): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n      )\n      (_bn1): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        96, 4, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        4, 96, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (2): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n      )\n      (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        144, 6, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        6, 144, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (3): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n      )\n      (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        144, 6, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        6, 144, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (4): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n      )\n      (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        240, 10, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        10, 240, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (5): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n      )\n      (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        240, 10, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        10, 240, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (6): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n      )\n      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        480, 20, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        20, 480, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (7): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n      )\n      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        480, 20, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        20, 480, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (8): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n      )\n      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        480, 20, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        20, 480, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (9): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n      )\n      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        672, 28, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        28, 672, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (10): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n      )\n      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        672, 28, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        28, 672, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (11): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n      )\n      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        672, 28, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        28, 672, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (12): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n      )\n      (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (13): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n      )\n      (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (14): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n      )\n      (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (15): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n      )\n      (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(320, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n  )\n  (_conv_head): Conv2dStaticSamePadding(\n    320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n    (static_padding): Identity()\n  )\n  (_bn1): BatchNorm2d(1280, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n  (_dropout): Dropout(p=0.2, inplace=False)\n  (_fc): Linear(in_features=1280, out_features=21, bias=True)\n  (_swish): MemoryEfficientSwish()\n)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}