{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "from efficientnet import EfficientNet_B0\n",
    "\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import average_precision_score\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "from voc_dataloader import VocDataset, VOC_CLASSES\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def train_classifier(train_loader, classifier, criterion, optimizer):\n",
    "    classifier.train()\n",
    "    loss_ = 0.0\n",
    "    losses = []\n",
    "    for i, (images, labels, _) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = classifier(images)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss)\n",
    "    return torch.stack(losses).mean().item()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def test_classifier(test_loader, classifier, criterion, print_ind_classes=True, print_total=True):\n",
    "    classifier.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        y_true = np.zeros((0,21))\n",
    "        y_score = np.zeros((0,21))\n",
    "        for i, (images, labels, _) in enumerate(test_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            logits = classifier(images)\n",
    "            y_true = np.concatenate((y_true, labels.cpu().numpy()), axis=0)\n",
    "            y_score = np.concatenate((y_score, logits.cpu().numpy()), axis=0)\n",
    "            loss = criterion(logits, labels)\n",
    "            losses.append(loss.item())\n",
    "        aps = []\n",
    "        # ignore first class which is background\n",
    "        for i in range(1, y_true.shape[1]):\n",
    "            ap = average_precision_score(y_true[:, i], y_score[:, i])\n",
    "            if print_ind_classes:\n",
    "                print('-------  Class: {:<12}     AP: {:>8.4f}  -------'.format(VOC_CLASSES[i], ap))\n",
    "            aps.append(ap)\n",
    "\n",
    "        mAP = np.mean(aps)\n",
    "        test_loss = np.mean(losses)\n",
    "        if print_total:\n",
    "            print('mAP: {0:.4f}'.format(mAP))\n",
    "            print('Avg loss: {}'.format(test_loss))\n",
    "\n",
    "    return mAP, test_loss, aps"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def plot_losses(train, val, test_frequency, num_epochs):\n",
    "    plt.plot(train, label=\"train\")\n",
    "    indices = [i for i in range(num_epochs) if ((i+1)%test_frequency == 0 or i ==0)]\n",
    "    plt.plot(indices, val, label=\"val\")\n",
    "    plt.title(\"Loss Plot\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_mAP(train, val, test_frequency, num_epochs):\n",
    "    indices = [i for i in range(num_epochs) if ((i+1)%test_frequency == 0 or i ==0)]\n",
    "    plt.plot(indices, train, label=\"train\")\n",
    "    plt.plot(indices, val, label=\"val\")\n",
    "    plt.title(\"mAP Plot\")\n",
    "    plt.ylabel(\"mAP\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def train(classifier, num_epochs, train_loader, val_loader, criterion, optimizer, test_frequency=5):\n",
    "    train_losses = []\n",
    "    train_mAPs = []\n",
    "    val_losses = []\n",
    "    val_mAPs = []\n",
    "\n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        print(\"Starting epoch number \" + str(epoch))\n",
    "        train_loss = train_classifier(train_loader, classifier, criterion, optimizer)\n",
    "        train_losses.append(train_loss)\n",
    "        print(\"Loss for Training on Epoch \" +str(epoch) + \" is \"+ str(train_loss))\n",
    "        if(epoch%test_frequency==0 or epoch==1):\n",
    "            mAP_train, _, _ = test_classifier(train_loader, classifier, criterion, False, False)\n",
    "            train_mAPs.append(mAP_train)\n",
    "            mAP_val, val_loss, _ = test_classifier(val_loader, classifier, criterion)\n",
    "            print('Evaluating classifier')\n",
    "            print(\"Mean Precision Score for Testing on Epoch \" +str(epoch) + \" is \"+ str(mAP_val))\n",
    "            val_losses.append(val_loss)\n",
    "            val_mAPs.append(mAP_val)\n",
    "\n",
    "    return classifier, train_losses, val_losses, train_mAPs, val_mAPs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\498_working\\efficientN\\voc_dataloader.py:109: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(names), np.array(labels).astype(np.float32), np.array(box_indices), label_order\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std= [0.229, 0.224, 0.225])\n",
    "\n",
    "# Use random crop and flip for training\n",
    "train_transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224,scale = (0.3,1.0)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize\n",
    "        ])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "            transforms.Resize(224),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "\n",
    "ds_train = VocDataset('../VOCdevkit_2007/VOC2007/','train',train_transform)\n",
    "ds_val = VocDataset('../VOCdevkit_2007/VOC2007/','val',test_transform)\n",
    "ds_test = VocDataset('../VOCdevkit_2007/VOC2007test/','test', test_transform)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "num_epochs = 60\n",
    "test_frequency = 5\n",
    "batch_size = 1\n",
    "# set a higher number of workers for better performance\n",
    "num_workers = 8\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=ds_train,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True,\n",
    "                                            pin_memory=True,\n",
    "                                            num_workers=num_workers)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=ds_val,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True,\n",
    "                                            pin_memory=True,\n",
    "                                            num_workers=num_workers)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=ds_test,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=False,\n",
    "                                               pin_memory=True,\n",
    "                                               num_workers=num_workers)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Efficient Net B0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch number 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-8-a50a42bb8a63>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[0mclassifier\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_losses\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mval_losses\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_mAPs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mval_mAPs\u001B[0m \u001B[1;33m=\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 8\u001B[1;33m     \u001B[0mtrain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mclassifier\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnum_epochs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_loader\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mval_loader\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtest_frequency\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      9\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-5-a94e263e2ce6>\u001B[0m in \u001B[0;36mtrain\u001B[1;34m(classifier, num_epochs, train_loader, val_loader, criterion, optimizer, test_frequency)\u001B[0m\n\u001B[0;32m      7\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mepoch\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mnum_epochs\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m         \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Starting epoch number \"\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mepoch\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 9\u001B[1;33m         \u001B[0mtrain_loss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrain_classifier\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_loader\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mclassifier\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     10\u001B[0m         \u001B[0mtrain_losses\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_loss\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     11\u001B[0m         \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Loss for Training on Epoch \"\u001B[0m \u001B[1;33m+\u001B[0m\u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mepoch\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;34m\" is \"\u001B[0m\u001B[1;33m+\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_loss\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-2-da23efa5bb0b>\u001B[0m in \u001B[0;36mtrain_classifier\u001B[1;34m(train_loader, classifier, criterion, optimizer)\u001B[0m\n\u001B[0;32m      4\u001B[0m     \u001B[0mlosses\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mimages\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabels\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_loader\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m         \u001B[0mimages\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabels\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mimages\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabels\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      7\u001B[0m         \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m         \u001B[0mlogits\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mclassifier\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimages\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "classifier = EfficientNet_B0().to(device)\n",
    "\n",
    "criterion = nn.MultiLabelSoftMarginLoss()\n",
    "#optimizer = torch.optim.SGD(classifier.parameters(), lr=0.01, momentum=0.9)\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-4)\n",
    "\n",
    "classifier, train_losses, val_losses, train_mAPs, val_mAPs = \\\n",
    "    train(classifier, num_epochs, train_loader, val_loader, criterion, optimizer, test_frequency)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "EfficientNet_B0(\n  (stage1): Conv3x3(\n    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n  )\n  (stage2): MBConv(\n    (block_se1): Sequential(\n      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n      (1): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      (2): Swish()\n      (3): AdaptiveAvgPool2d(output_size=1)\n      (4): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n      (5): Swish()\n      (6): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n      (7): Sigmoid()\n    )\n    (block_se2): Sequential(\n      (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (1): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (stage3): Sequential(\n    (0): MBConv(\n      (expand): Sequential(\n        (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (2): Swish()\n      )\n      (block_se1): Sequential(\n        (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n        (1): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (2): Swish()\n        (3): AdaptiveAvgPool2d(output_size=1)\n        (4): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n        (5): Swish()\n        (6): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n        (7): Sigmoid()\n      )\n      (block_se2): Sequential(\n        (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): MBConv(\n      (expand): Sequential(\n        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (2): Swish()\n      )\n      (block_se1): Sequential(\n        (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n        (1): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (2): Swish()\n        (3): AdaptiveAvgPool2d(output_size=1)\n        (4): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n        (5): Swish()\n        (6): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n        (7): Sigmoid()\n      )\n      (block_se2): Sequential(\n        (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n  )\n  (stage4): Sequential(\n    (0): MBConv(\n      (expand): Sequential(\n        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (2): Swish()\n      )\n      (block_se1): Sequential(\n        (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n        (1): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (2): Swish()\n        (3): AdaptiveAvgPool2d(output_size=1)\n        (4): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n        (5): Swish()\n        (6): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n        (7): Sigmoid()\n      )\n      (block_se2): Sequential(\n        (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(40, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): MBConv(\n      (expand): Sequential(\n        (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (2): Swish()\n      )\n      (block_se1): Sequential(\n        (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n        (1): BatchNorm2d(240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (2): Swish()\n        (3): AdaptiveAvgPool2d(output_size=1)\n        (4): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n        (5): Swish()\n        (6): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n        (7): Sigmoid()\n      )\n      (block_se2): Sequential(\n        (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(40, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n  )\n  (stage5): Sequential(\n    (0): MBConv(\n      (expand): Sequential(\n        (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (2): Swish()\n      )\n      (block_se1): Sequential(\n        (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)\n        (1): BatchNorm2d(240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (2): Swish()\n        (3): AdaptiveAvgPool2d(output_size=1)\n        (4): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n        (5): Swish()\n        (6): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n        (7): Sigmoid()\n      )\n      (block_se2): Sequential(\n        (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): MBConv(\n      (expand): Sequential(\n        (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (2): Swish()\n      )\n      (block_se1): Sequential(\n        (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n        (1): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (2): Swish()\n        (3): AdaptiveAvgPool2d(output_size=1)\n        (4): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n        (5): Swish()\n        (6): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n        (7): Sigmoid()\n      )\n      (block_se2): Sequential(\n        (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (2): MBConv(\n      (expand): Sequential(\n        (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (2): Swish()\n      )\n      (block_se1): Sequential(\n        (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n        (1): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (2): Swish()\n        (3): AdaptiveAvgPool2d(output_size=1)\n        (4): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n        (5): Swish()\n        (6): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n        (7): Sigmoid()\n      )\n      (block_se2): Sequential(\n        (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n  )\n  (stage6): Sequential(\n    (0): MBConv(\n      (expand): Sequential(\n        (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (2): Swish()\n      )\n      (block_se1): Sequential(\n        (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=480, bias=False)\n        (1): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (2): Swish()\n        (3): AdaptiveAvgPool2d(output_size=1)\n        (4): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n        (5): Swish()\n        (6): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n        (7): Sigmoid()\n      )\n      (block_se2): Sequential(\n        (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): MBConv(\n      (expand): Sequential(\n        (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (2): Swish()\n      )\n      (block_se1): Sequential(\n        (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n        (1): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (2): Swish()\n        (3): AdaptiveAvgPool2d(output_size=1)\n        (4): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n        (5): Swish()\n        (6): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n        (7): Sigmoid()\n      )\n      (block_se2): Sequential(\n        (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (2): MBConv(\n      (expand): Sequential(\n        (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (2): Swish()\n      )\n      (block_se1): Sequential(\n        (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n        (1): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (2): Swish()\n        (3): AdaptiveAvgPool2d(output_size=1)\n        (4): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n        (5): Swish()\n        (6): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n        (7): Sigmoid()\n      )\n      (block_se2): Sequential(\n        (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n  )\n  (stage7): Sequential(\n    (0): MBConv(\n      (expand): Sequential(\n        (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (2): Swish()\n      )\n      (block_se1): Sequential(\n        (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n        (1): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (2): Swish()\n        (3): AdaptiveAvgPool2d(output_size=1)\n        (4): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n        (5): Swish()\n        (6): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n        (7): Sigmoid()\n      )\n      (block_se2): Sequential(\n        (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): MBConv(\n      (expand): Sequential(\n        (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (2): Swish()\n      )\n      (block_se1): Sequential(\n        (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n        (1): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (2): Swish()\n        (3): AdaptiveAvgPool2d(output_size=1)\n        (4): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n        (5): Swish()\n        (6): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n        (7): Sigmoid()\n      )\n      (block_se2): Sequential(\n        (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (2): MBConv(\n      (expand): Sequential(\n        (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (2): Swish()\n      )\n      (block_se1): Sequential(\n        (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n        (1): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (2): Swish()\n        (3): AdaptiveAvgPool2d(output_size=1)\n        (4): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n        (5): Swish()\n        (6): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n        (7): Sigmoid()\n      )\n      (block_se2): Sequential(\n        (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (3): MBConv(\n      (expand): Sequential(\n        (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (2): Swish()\n      )\n      (block_se1): Sequential(\n        (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n        (1): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n        (2): Swish()\n        (3): AdaptiveAvgPool2d(output_size=1)\n        (4): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n        (5): Swish()\n        (6): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n        (7): Sigmoid()\n      )\n      (block_se2): Sequential(\n        (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n  )\n  (stage8): MBConv(\n    (expand): Sequential(\n      (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (1): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      (2): Swish()\n    )\n    (block_se1): Sequential(\n      (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=1152, bias=False)\n      (1): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n      (2): Swish()\n      (3): AdaptiveAvgPool2d(output_size=1)\n      (4): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n      (5): Swish()\n      (6): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n      (7): Sigmoid()\n    )\n    (block_se2): Sequential(\n      (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (1): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (conv_f): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n  (avgpool): AdaptiveAvgPool2d(output_size=1)\n  (bn_f): BatchNorm2d(1280, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n  (dropout_f): Dropout(p=0.2, inplace=False)\n  (fc): Linear(in_features=1280, out_features=21, bias=True)\n  (swish): Swish()\n)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}